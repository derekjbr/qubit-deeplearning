{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import everything\n",
    "import numpy as np\n",
    "# from gym.utils import seeding\n",
    "# from gym.spaces import Discrete, Tuple, Box\n",
    "# import gym\n",
    "from qiskit.quantum_info import state_fidelity\n",
    "from qiskit import *\n",
    "from numpy.linalg import matrix_power\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set globally used variables\n",
    "n = 10**6\n",
    "k = 10\n",
    "\n",
    "GATES = {\n",
    "    0: np.array([[1, 1], [1, -1]]) * 1/np.sqrt(2), # H\n",
    "    1: np.array([[1, 0], [0, np.exp(1j * np.pi / 4)]]), # T\n",
    "    # 2: np.array([[0, 1], [1, 0]]), # X\n",
    "    2: np.array([[1, 0], [0, 1]]) # I\n",
    "}\n",
    "\n",
    "thetas = np.array(pd.cut(np.linspace(0, np.pi, k), k, precision=10, include_lowest=True))\n",
    "thetas[0] = pd.Interval(0, thetas[0].right, closed='both')\n",
    "phis = np.array(pd.cut(np.linspace(0, 2*np.pi, 2*k), 2*k,  precision=10, include_lowest=True))\n",
    "phis[0] = pd.Interval(0, phis[0].right, closed='both')\n",
    "\n",
    "states = [(i, j) for i in range(len(thetas)) for j in range(len(phis))]\n",
    "values = np.zeros(len(thetas) * len(phis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_circuit(n):\n",
    "    s = np.array([1, 0])\n",
    "    ht = GATES[0] @ GATES[1]\n",
    "    return matrix_power(ht, n) @ s\n",
    "\n",
    "def statevector_to_angles(state):\n",
    "    svp = [abs(state[0])*np.exp(1j * np.angle(state[0])), abs(state[1])*np.exp(1j * np.angle(state[1]))]\n",
    "    svp /= np.exp(1j * np.angle(state[0]))\n",
    "    theta = 2 * np.arccos(abs(svp[0]))\n",
    "    phi = np.angle(svp[1])\n",
    "    if (phi < 0): phi += 2*np.pi\n",
    "    return theta, phi\n",
    "    # return np.cos(theta / 2) * np.array([1,0]) + np.exp(1j * phi) * np.sin(theta / 2) * np.array([0, 1])\n",
    "\n",
    "def statevector_to_bloch_reg(state):\n",
    "    theta, phi = statevector_to_angles(state)\n",
    "\n",
    "    # take into consideration the poles\n",
    "    for i in range(len(thetas)):\n",
    "        if (theta in thetas[i]):\n",
    "            theta_reg = i\n",
    "    for i in range(len(phis)):\n",
    "        if (phi in phis[i]):\n",
    "            phi_reg = i\n",
    "\n",
    "    if (theta_reg == 0):\n",
    "        theta_reg = phi_reg = 0\n",
    "    if (theta_reg == len(thetas)-1):\n",
    "        theta_reg = len(thetas)-1\n",
    "        phi_reg = len(phis)-1\n",
    "    return (theta_reg, phi_reg)\n",
    "\n",
    "def random_state_in_reg(reg):\n",
    "    if (reg[0] == 0 or reg[0] == len(thetas)-1):\n",
    "        phi = np.random.uniform(0, 2*np.pi)\n",
    "    else:\n",
    "        phi = np.random.uniform(phis[reg[1]].left, phis[reg[1]].right)\n",
    "    theta = np.random.uniform(thetas[reg[0]].left, thetas[reg[0]].right)\n",
    "    return np.cos(theta / 2) * np.array([1,0]) + np.exp(1j * phi) * np.sin(theta / 2) * np.array([0, 1])\n",
    "\n",
    "def statevector_to_bloch_point(state):\n",
    "    svp = [abs(state[0])*np.exp(1j * np.angle(state[0])), abs(state[1])*np.exp(1j * np.angle(state[1]))]\n",
    "    svp /= np.exp(1j * np.angle(svp[0]))\n",
    "    theta = 2 * np.arccos(abs(svp[0]))\n",
    "    phi = np.angle(svp[1])\n",
    "    return np.sin(theta)*np.cos(phi), np.sin(theta)*np.sin(phi), np.cos(theta)\n",
    "\n",
    "def random_unitary(dim):\n",
    "  # follows the algorithm in https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "  Z = np.array([np.random.normal(0, 1) + np.random.normal(0, 1) * 1j for _ in range(dim ** 2)]).reshape(dim, dim)\n",
    "  Q, R = np.linalg.qr(Z)\n",
    "  diag = np.diagonal(R)\n",
    "  lamb = np.diag(diag) / np.absolute(diag)\n",
    "  unitary = np.matmul(Q, lamb)\n",
    "  assert np.allclose(unitary.conj().T @ unitary, np.eye(dim))\n",
    "  return unitary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = generate_target_circuit(n=n)\n",
    "goal_region = statevector_to_bloch_reg(goal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This defines the reward function. Reward to network iff it is in the goal bloch region\n",
    "def R(state):\n",
    "    if (state == goal_region):\n",
    "        return 1\n",
    "        # if (action <= len(GATES) - 2):\n",
    "        #     return 0\n",
    "        # else:\n",
    "        #     return 0.1 # to encourage using identity\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(policy, max_length_of_episode):\n",
    "    episode = []\n",
    "    t = 0 # time t\n",
    "    current_state = 0 # The index of a randomly chosen region\n",
    "    while t < max_length_of_episode and current_state != states.index(goal_region):\n",
    "        timestep = []\n",
    "        timestep.append(current_state) # Add the current state to the timestep\n",
    "\n",
    "        action = np.random.choice(3, p=policy[current_state]) # Choose a random action under the policy based on the policies distribution\n",
    "\n",
    "        r_state = random_state_in_reg(states[current_state]) # Choose a random region inside the state. This simulates the probablistic distribution\n",
    "        current_state = states.index(statevector_to_bloch_reg(GATES[action] @ r_state)) # Apply the action and get our next state\n",
    "\n",
    "        timestep.append(action) # Add that action to the timestep\n",
    "        timestep.append(R(states[current_state])) # Add the reward of our future state\n",
    "        episode.append(timestep) # Append the timestep to the episode\n",
    "        t += 1\n",
    "    \n",
    "    return episode\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(inital_state, terminal, eplison=0.01, gamma = 0.99,step_size = 1, episode_count=10000):\n",
    "    Q = np.ones([len(states), len(GATES)])\n",
    "    print(\"Terminal State : {0} Terminal State Index : {1}\".format(terminal, states.index(terminal)))\n",
    "    Q[states.index(terminal)] = np.zeros(len(GATES))\n",
    "\n",
    "    for _ in range(episode_count):\n",
    "        S = inital_state #Let this be the coordinates, (0,0), and not the index\n",
    "        count = 0\n",
    "        while S != terminal and count < 200: #Look until we reach our terminal state. Might go on forever \n",
    "            S_index = states.index(S) # Let S_index represent the states index in the Q array\n",
    "            A = np.argmax(Q[S_index]) # Get the argMax(S, a) = a of the current state \n",
    "            S_R = random_state_in_reg(S)\n",
    "\n",
    "            S_1 = statevector_to_bloch_reg(GATES[A] @ random_state_in_reg(S)) # Apply action to get S_t+1\n",
    "            S_1_index = states.index(S_1) # Let S_1_index represent the index of S_1 in Q\n",
    "            r = R(S_1) # Find the reward of the S_t+1\n",
    "            Q[S_index][A] = Q[S_index][A] + (step_size * (r + (gamma*Q[S_1_index].max()) - Q[S_index][A]))\n",
    "\n",
    "            S = S_1\n",
    "\n",
    "            count += 1\n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal State : (5, 17) Terminal State Index : 117\n",
      "converged\n",
      "[[0, 1, 1, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "n = 10**6\n",
    "goal = generate_target_circuit(n=n)\n",
    "goal_reg = statevector_to_bloch_reg(goal)\n",
    "\n",
    "#Generate and train policy\n",
    "policy = q_learning((0,0), goal_reg, step_size=0.9,episode_count=1000)\n",
    "#print(policy)\n",
    "\n",
    "optimal_programs = []\n",
    "converged = False\n",
    "while not converged:\n",
    "    s = random_state_in_reg((0, 0))\n",
    "    prog = []\n",
    "    counter = 0\n",
    "    while counter < 30:\n",
    "        action = np.argmax(policy[states.index(statevector_to_bloch_reg(s))])\n",
    "        next_s = GATES[action] @ s\n",
    "        prog.append(action)\n",
    "        # next_s = random_state_in_reg(statevector_to_bloch_reg(next_s))\n",
    "        s = next_s\n",
    "        counter += 1\n",
    "        if (statevector_to_bloch_reg(s) == goal_reg):\n",
    "            print('converged')\n",
    "            converged = True\n",
    "            break\n",
    "    \n",
    "optimal_programs.append(prog)\n",
    "\n",
    "print(optimal_programs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9976538146186626\n"
     ]
    }
   ],
   "source": [
    "fidelities = []\n",
    "prog = [0, 1, 1, 0, 1]\n",
    "for i in range(100000):\n",
    "    s = np.array([1, 0])\n",
    "    s = random_state_in_reg((0,0))\n",
    "    for a in prog:\n",
    "        s = GATES[a] @ s\n",
    "    f = state_fidelity(s, goal)\n",
    "    if (statevector_to_bloch_reg(s) == goal_reg):\n",
    "        fidelities.append(f)\n",
    "        break\n",
    "    # print(goal, s)\n",
    "print(np.average(fidelities))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a476930057e67fb0f6b7edcbcc08289d2ba2459b03a5fa4101c6d153d3bda19"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('.research': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
