{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import everything\n",
    "import numpy as np\n",
    "# from gym.utils import seeding\n",
    "# from gym.spaces import Discrete, Tuple, Box\n",
    "# import gym\n",
    "from qiskit.quantum_info import state_fidelity\n",
    "from qiskit import *\n",
    "from numpy.linalg import matrix_power\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "#Set globally used variables\n",
    "n = 10**6\n",
    "k = 30\n",
    "\n",
    "GATES = {\n",
    "    0: np.array([[1, 1], [1, -1]]) * 1/np.sqrt(2), # H\n",
    "    1: np.array([[1, 0], [0, np.exp(1j * np.pi / 4)]]), # T\n",
    "    # 2: np.array([[0, 1], [1, 0]]), # X\n",
    "    2: np.array([[1, 0], [0, 1]]) # I\n",
    "}\n",
    "\n",
    "thetas = np.array(pd.cut(np.linspace(0, np.pi, k), k, precision=10, include_lowest=True))\n",
    "thetas[0] = pd.Interval(0, thetas[0].right, closed='both')\n",
    "phis = np.array(pd.cut(np.linspace(0, 2*np.pi, 2*k), 2*k,  precision=10, include_lowest=True))\n",
    "phis[0] = pd.Interval(0, phis[0].right, closed='both')\n",
    "\n",
    "states = [(i, j) for i in range(len(thetas)) for j in range(len(phis))]\n",
    "values = np.zeros(len(thetas) * len(phis))\n",
    "\n",
    "print(len(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_circuit(n):\n",
    "    s = np.array([1, 0])\n",
    "    ht = GATES[0] @ GATES[1]\n",
    "    return matrix_power(ht, n) @ s\n",
    "\n",
    "def statevector_to_angles(state):\n",
    "    svp = [abs(state[0])*np.exp(1j * np.angle(state[0])), abs(state[1])*np.exp(1j * np.angle(state[1]))]\n",
    "    svp /= np.exp(1j * np.angle(state[0]))\n",
    "    theta = 2 * np.arccos(abs(svp[0]))\n",
    "    phi = np.angle(svp[1])\n",
    "    if (phi < 0): phi += 2*np.pi\n",
    "    return theta, phi\n",
    "    # return np.cos(theta / 2) * np.array([1,0]) + np.exp(1j * phi) * np.sin(theta / 2) * np.array([0, 1])\n",
    "\n",
    "def statevector_to_bloch_reg(state):\n",
    "    theta, phi = statevector_to_angles(state)\n",
    "\n",
    "    # take into consideration the poles\n",
    "    for i in range(len(thetas)):\n",
    "        if (theta in thetas[i]):\n",
    "            theta_reg = i\n",
    "    for i in range(len(phis)):\n",
    "        if (phi in phis[i]):\n",
    "            phi_reg = i\n",
    "\n",
    "    if (theta_reg == 0):\n",
    "        theta_reg = phi_reg = 0\n",
    "    if (theta_reg == len(thetas)-1):\n",
    "        theta_reg = len(thetas)-1\n",
    "        phi_reg = len(phis)-1\n",
    "    return (theta_reg, phi_reg)\n",
    "\n",
    "def random_state_in_reg(reg):\n",
    "    if (reg[0] == 0 or reg[0] == len(thetas)-1):\n",
    "        phi = np.random.uniform(0, 2*np.pi)\n",
    "    else:\n",
    "        phi = np.random.uniform(phis[reg[1]].left, phis[reg[1]].right)\n",
    "    theta = np.random.uniform(thetas[reg[0]].left, thetas[reg[0]].right)\n",
    "    return np.cos(theta / 2) * np.array([1,0]) + np.exp(1j * phi) * np.sin(theta / 2) * np.array([0, 1])\n",
    "\n",
    "def statevector_to_bloch_point(state):\n",
    "    svp = [abs(state[0])*np.exp(1j * np.angle(state[0])), abs(state[1])*np.exp(1j * np.angle(state[1]))]\n",
    "    svp /= np.exp(1j * np.angle(svp[0]))\n",
    "    theta = 2 * np.arccos(abs(svp[0]))\n",
    "    phi = np.angle(svp[1])\n",
    "    return np.sin(theta)*np.cos(phi), np.sin(theta)*np.sin(phi), np.cos(theta)\n",
    "\n",
    "def random_unitary(dim):\n",
    "  # follows the algorithm in https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "  Z = np.array([np.random.normal(0, 1) + np.random.normal(0, 1) * 1j for _ in range(dim ** 2)]).reshape(dim, dim)\n",
    "  Q, R = np.linalg.qr(Z)\n",
    "  diag = np.diagonal(R)\n",
    "  lamb = np.diag(diag) / np.absolute(diag)\n",
    "  unitary = np.matmul(Q, lamb)\n",
    "  assert np.allclose(unitary.conj().T @ unitary, np.eye(dim))\n",
    "  return unitary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = generate_target_circuit(n=n)\n",
    "goal_region = statevector_to_bloch_reg(goal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This defines the reward function. Reward to network iff it is in the goal bloch region\n",
    "def R(state, action):\n",
    "    if (state == goal_reg):\n",
    "        return 1\n",
    "        # if (action <= len(GATES) - 2):\n",
    "        #     return 0\n",
    "        # else:\n",
    "        #     return 0.1 # to encourage using identity\n",
    "    else:\n",
    "        return -0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This defines the reward function. Reward to network iff it is in the goal bloch region\n",
    "def R(state, count):\n",
    "    if (state == goal_region):\n",
    "        return 1\n",
    "        # if (action <= len(GATES) - 2):\n",
    "        #     return 0\n",
    "        # else:\n",
    "        #     return 0.1 # to encourage using identity\n",
    "    else:\n",
    "        return -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(inital_state, terminal, eplison=0.01, gamma = 0.99,alpha = 1, episode_count=10000):\n",
    "    # Check random values \n",
    "    Q = np.ones([len(states), len(GATES)])\n",
    "    print(\"Terminal State : {0} Terminal State Index : {1}\".format(terminal, states.index(terminal)))\n",
    "    Q[states.index(terminal)] = np.zeros(len(GATES))\n",
    "\n",
    "    for _ in range(episode_count):\n",
    "        S = inital_state #Let this be the coordinates, (0,0), and not the index\n",
    "        count = 0\n",
    "        while S != terminal and count < 50: #Look until we reach our terminal state. Might go on forever \n",
    "            S_index = states.index(S) # Let S_index represent the states index in the Q array\n",
    "\n",
    "            # Initally H\n",
    "            A = np.argmax(Q[S_index]) # Get the argMax(S, a) = a of the current state \n",
    "\n",
    "            S_1 = statevector_to_bloch_reg(GATES[A] @ random_state_in_reg(S)) # Apply action to get S_t+1\n",
    "            S_1_index = states.index(S_1) # Let S_1_index represent the index of S_1 in Q\n",
    "            r = R(S_1, count) # Find the reward of the S_t+1\n",
    "            Q[S_index][A] = Q[S_index][A] + (alpha * (r + (gamma*Q[S_1_index].max()) - Q[S_index][A]))\n",
    "\n",
    "            S = S_1\n",
    "\n",
    "            count += 1\n",
    "    return Q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal State : (15, 53) Terminal State Index : 953\n",
      "[[-0.18152186 -0.24858213 -0.24858213]\n",
      " [ 1.          1.          1.        ]\n",
      " [ 1.          1.          1.        ]\n",
      " ...\n",
      " [ 1.          1.          1.        ]\n",
      " [ 1.          1.          1.        ]\n",
      " [-0.06097365 -0.090667   -0.090667  ]]\n"
     ]
    }
   ],
   "source": [
    "n = 10**6\n",
    "goal = generate_target_circuit(n=n)\n",
    "goal_reg = statevector_to_bloch_reg(goal)\n",
    "\n",
    "#Generate and train policy\n",
    "policy = q_learning((0,0), goal_reg, alpha=0.8,episode_count=1000)\n",
    "for i in policy:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal State : (15, 53) Terminal State Index : 953\n",
      "[[-0.73303694 -0.85547514 -0.85547514]\n",
      " [ 1.          1.          1.        ]\n",
      " [ 1.          1.          1.        ]\n",
      " ...\n",
      " [ 1.          1.          1.        ]\n",
      " [ 1.          1.          1.        ]\n",
      " [-0.46379335 -0.55690289 -0.55690289]]\n",
      "Policy has finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dbran\\Coding\\Python\\qubit-deeplearning\\mdp_qL.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dbran/Coding/Python/qubit-deeplearning/mdp_qL.ipynb#ch0000009?line=19'>20</a>\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(policy[states\u001b[39m.\u001b[39mindex(statevector_to_bloch_reg(s))])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dbran/Coding/Python/qubit-deeplearning/mdp_qL.ipynb#ch0000009?line=20'>21</a>\u001b[0m next_s \u001b[39m=\u001b[39m GATES[action] \u001b[39m@\u001b[39m s\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dbran/Coding/Python/qubit-deeplearning/mdp_qL.ipynb#ch0000009?line=21'>22</a>\u001b[0m prog\u001b[39m.\u001b[39;49mappend(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dbran/Coding/Python/qubit-deeplearning/mdp_qL.ipynb#ch0000009?line=22'>23</a>\u001b[0m \u001b[39m# next_s = random_state_in_reg(statevector_to_bloch_reg(next_s))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dbran/Coding/Python/qubit-deeplearning/mdp_qL.ipynb#ch0000009?line=23'>24</a>\u001b[0m s \u001b[39m=\u001b[39m next_s\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 10**6\n",
    "goal = generate_target_circuit(n=n)\n",
    "goal_reg = statevector_to_bloch_reg(goal)\n",
    "\n",
    "#Generate and train policy\n",
    "policy = q_learning((0,0), goal_reg, alpha=0.8,episode_count=2000)\n",
    "\n",
    "\n",
    "print(\"Policy has finished\")\n",
    "\n",
    "#Noahs\n",
    "optimal_programs = []\n",
    "for i in range(10):\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        s = random_state_in_reg((0, 0))\n",
    "        prog = []\n",
    "        counter = 0\n",
    "        while counter < 30:\n",
    "            action = np.argmax(policy[states.index(statevector_to_bloch_reg(s))])\n",
    "            next_s = GATES[action] @ s\n",
    "            prog.append(action)\n",
    "            # next_s = random_state_in_reg(statevector_to_bloch_reg(next_s))\n",
    "            s = next_s\n",
    "            counter += 1\n",
    "            if (statevector_to_bloch_reg(s) == goal_reg):\n",
    "                print('converged')\n",
    "                converged = True\n",
    "                break\n",
    "        \n",
    "    optimal_programs.append(prog)\n",
    "optimal_programs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974210798618732\n"
     ]
    }
   ],
   "source": [
    "fidelities = []\n",
    "prog = [0, 1, 1, 0, 1]\n",
    "for i in range(1):\n",
    "    s = np.array([1, 0])\n",
    "    s = random_state_in_reg((0,0))\n",
    "    for a in prog:\n",
    "        s = GATES[a] @ s\n",
    "    f = state_fidelity(s, goal)\n",
    "    fidelities.append(f)\n",
    "    break\n",
    "    # print(goal, s)\n",
    "print(np.average(fidelities))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a476930057e67fb0f6b7edcbcc08289d2ba2459b03a5fa4101c6d153d3bda19"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('.research': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
